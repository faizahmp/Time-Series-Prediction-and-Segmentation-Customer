# -*- coding: utf-8 -*-
"""Kalbe Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oe3tAZVl-S8oeGxHR2zMlmCZqJwpsSKS

# Libraries
"""

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt
from statsmodels.tsa.arima.model import ARIMA
from pandas.plotting import autocorrelation_plot

import warnings
warnings.filterwarnings('ignore')

"""# Read Data"""

df_customer = pd.read_csv('/content/Case Study - Customer.csv', delimiter=';')
df_product = pd.read_csv('/content/Case Study - Product.csv', delimiter=';')
df_store = pd.read_csv('/content/Case Study - Store.csv', delimiter=';')
df_transaction = pd.read_csv('/content/Case Study - Transaction.csv', delimiter=';')
df_customer.shape, df_product.shape, df_store.shape, df_transaction.shape

"""# Data Cleaning"""

df_customer.head()

df_customer.info()

df_customer.isnull().sum()

df_product.head()

df_product.info()

df_store.head()

df_store.info()

df_transaction.head()

df_transaction.info()

"""## Data Types"""

# data cleaning customer
df_customer['Income'] = df_customer['Income'].replace('[,]','.', regex=True).astype('float')

# data cleaning store
df_store['Latitude'] = df_store['Latitude'].replace('[,]','.', regex=True).astype('float')
df_store['Longitude'] = df_store['Longitude'].replace('[,]','.', regex=True).astype('float')

# data cleaning transaction
df_transaction['Date'] = pd.to_datetime(df_transaction['Date'])

"""## Merge All Data"""

df_merge = pd.merge(df_transaction, df_customer, on=['CustomerID'])
df_merge = pd.merge(df_merge, df_product.drop(columns=['Price']), on=['ProductID'])
df_merge = pd.merge(df_merge, df_store, on=['StoreID'])

df_merge.head()

df_transaction['TransactionID'].value_counts()

df_transaction[df_transaction['TransactionID'] == 'TR71313']

"""# Model Building

## Regression Model (Time Series)
"""

df_regresi = df_merge.groupby(['Date']).agg({
    'Qty' : 'sum'
}).reset_index()

df_regresi

decomposed = seasonal_decompose(df_regresi.set_index('Date'))

plt.figure(figsize=(8,8))

plt.subplot(311)
decomposed.trend.plot(ax=plt.gca())
plt.title('Trend')

plt.subplot(312)
decomposed.seasonal.plot(ax=plt.gca())
plt.title('Seasonality')

plt.subplot(313)
decomposed.resid.plot(ax=plt.gca())
plt.title('Residuals')

plt.tight_layout()

"""### Check Stationarity Data"""

from statsmodels.tsa.stattools import adfuller
result = adfuller(df_regresi['Qty'])
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
  print('\t%s: %.3f' % (key, value))

cut_off = round(df_regresi.shape[0]*0.8)
df_train = df_regresi[:cut_off]
df_test = df_regresi[cut_off:].reset_index(drop=True)
df_train.shape, df_test.shape

df_train

df_test

plt.figure(figsize=(15,5))
sns.lineplot(data=df_train, x=df_train['Date'], y=df_train['Qty'])
sns.lineplot(data=df_test, x=df_test['Date'], y=df_test['Qty'])

autocorrelation_plot(df_regresi['Qty'])

def rmse(y_actual, y_pred):
  print(f'RMSE Value {mean_squared_error(y_actual, y_pred)**0.5}')

def eval(y_actual, y_pred):
  rmse(y_actual, y_pred)
  print(f'MAE value {mean_absolute_error(y_actual, y_pred)}')

"""### ARIMA Model"""

# ARIMA MODEL
df_train = df_train.set_index('Date')
df_test = df_test.set_index('Date')

y = df_train['Qty']

ARIMAmodel = ARIMA(y, order=(40,2,1))
ARIMAmodel = ARIMAmodel.fit()

y_pred =  ARIMAmodel.get_forecast(len(df_test))

y_pred_df = y_pred.conf_int()
y_pred_df['predictions'] = ARIMAmodel.predict(start=y_pred_df.index[0], end=y_pred_df.index[-1])
y_pred_df.index = df_test.index
y_pred_out = y_pred_df['predictions']
eval(df_test['Qty'], y_pred_out)

plt.figure(figsize=(20,5))
plt.plot(df_train['Qty'])
plt.plot(df_test['Qty'], color='red')
plt.plot(y_pred_out, color='black', label='ARIMA Predictions')
plt.legend()

"""## Clustering"""

df_merge.head()

df_merge.info()

df_merge.corr()

correlation_matrix = df_merge.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('df_merge Correlation Matrix')
plt.show()

df_cluster = df_merge.groupby(['CustomerID']).agg({
    'TransactionID' : 'count',
    'Qty': 'sum'
}).reset_index()

df_cluster

data_cluster = df_cluster.drop(columns=['CustomerID'])
data_cluster_normalize = preprocessing.normalize(data_cluster)

data_cluster_normalize

"""### Determine the optimal K"""

#KMEANS
K = range(2,8)
fits = []
score = []

for k in K:
  model = KMeans(n_clusters=k, random_state=0, n_init='auto').fit(data_cluster_normalize)
  fits.append(model)
  score.append(silhouette_score(data_cluster_normalize, model.labels_, metric='euclidean'))
print(score)

plt.figure(figsize=(8,3))
plt.plot(list(range(2,8)), score, color='royalblue', marker='o', linewidth=2, markersize=12, markerfacecolor='m', markeredgecolor='m' )
plt.title('Score vs num of cluster',fontsize=18)
plt.xlabel('num of Cluster',fontsize=15)
plt.ylabel('Score',fontsize=15)
plt.show()

fits[1]

df_cluster['cluster_label'] = fits[1].labels_

df_cluster.groupby(['cluster_label']).agg({
    'CustomerID' : 'count',
    'TransactionID' : 'mean',
    'Qty' : 'mean'
})

cluster_0_cust = df_cluster.loc[df_cluster['cluster_label'] == 0, ['cluster_label', 'CustomerID']]
cluster_0_cust

# Scatter plot
plt.figure(figsize=(10, 6))
for label in df_cluster['cluster_label'].unique():
    cluster_data = df_cluster[df_cluster['cluster_label'] == label]
    plt.scatter(cluster_data['TransactionID'], cluster_data['Qty'], label=f'Cluster {label}')

plt.title('Cluster Visualization')
plt.xlabel('TransactionID')
plt.ylabel('Qty')
plt.legend()
plt.show()